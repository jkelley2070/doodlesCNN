{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages \n",
    "\n",
    "* PIL for images\n",
    "* torchvision for pytorch computer vision support \n",
    "* datasets -> loading dataset with huggingface dataset api\n",
    "* matplotlib for showing images\n",
    "* torch and its many packages and modules for nn\n",
    "* tqdm for progress bar when training and testing\n",
    "* numpy for math stuff\n",
    "* ray tune for hyperparameter otpimization\n",
    "* torch summary for model params count\n",
    "* typing for better clarity \n",
    "* CNN storing the cnn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/kelley/Desktop/drawAI/.env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "!pip -q install accelerate datasets evaluate torchvision Pillow 'transformers[torch]' tqdm ray tensorboardX torchmetrics torchsummary scikit-learn numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-26 21:17:44,796\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-04-26 21:17:44,845\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Subset\n",
    "import matplotlib as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "from ray import tune\n",
    "from tensorboardX import SummaryWriter\n",
    "from ray.tune.stopper import MaximumIterationStopper, TrialPlateauStopper\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torchmetrics import AveragePrecision\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torchsummary import summary\n",
    "from datasets import Dataset, DatasetDict\n",
    "from typing import List, Dict, Any\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset infromation \n",
    "* 10% Subset of Google's 50 million quickdraw dataset \n",
    "    * training -> 4.5 million\n",
    "    * validation -> .5 million \n",
    "    * test ->.5 million\n",
    "* link -> https://huggingface.co/datasets/Xenova/quickdraw-small?row=0\n",
    "* 28 x 28 images of drawings \n",
    "* 345 labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 4500000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 250000\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 250000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# mac gpu loading \n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# loading dataset into huggign face dataset dictionary/object\n",
    "dataset:DatasetDict = load_dataset('Xenova/quickdraw-small')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 batches tested for each, previously generated with random\n",
    "# example\n",
    "# batcha_selected_classes_10  = random.sample(range(0, 344), 10)\n",
    "batcha_selected_classes_10 = [121, 162, 120, 211, 43, 185, 177, 171, 232, 322]\n",
    "batcha_selected_classes_50 = [143, 149, 306, 233, 107, 196, 129, 186, 22, 227, 16, 253, 199, 194, 333, 122, 297, 307, 217, 42, 251, 158, 324, 124, 334, 270, 165, 78, 214, 289, 69, 169, 206, 133, 278, 262, 298, 54, 115, 272, 110, 215, 209, 172, 328, 5, 207, 14, 9, 51]\n",
    "batchb_selected_classes_10 = [223, 60, 237, 51, 234, 94, 53, 161, 300, 320]\n",
    "batchb_selected_classes_50 = [148, 30, 79, 253, 18, 6, 141, 116, 150, 273, 68, 339, 262, 302, 197, 56, 333, 182, 265, 323, 101, 226, 308, 155, 171, 199, 25, 28, 48, 10, 342, 66, 104, 318, 185, 330, 263, 257, 35, 296, 70, 228, 272, 316, 42, 69, 233, 127, 217, 169]\n",
    "batchc_selected_classes_10 = [302, 323, 107, 312, 184, 235, 179, 146, 38, 149]\n",
    "batchc_selected_classes_50 = [248, 32, 60, 128, 290, 77, 49, 282, 95, 176, 193, 72, 189, 226, 100, 107, 304, 308, 14, 287, 35, 250, 111, 297, 291, 112, 62, 168, 136, 11, 323, 327, 239, 88, 159, 120, 257, 240, 24, 16, 148, 160, 40, 96, 155, 53, 280, 36, 64, 151]\n",
    "\n",
    "# pytorch must have labels 0 - x, so using dict to create mapping\n",
    "# i.e 121::0, 162:1, using this mapping later\n",
    "def create_label_mapping(selected_classes: List[int]) -> Dict[int, int]:\n",
    "    return {old_label: new_label for new_label, old_label in enumerate(selected_classes)}\n",
    "\n",
    "# function to create a flag in the dataset to filter based on\n",
    "# there are better ways to do this but trying to use pretrained models later\n",
    "# and want it as flexible as possible\n",
    "def transform_example(example: Dict[str, Any], selected_classes: List[int], label_mapping: Dict[int, int]) -> Dict[str, Any]:\n",
    "    if example['label'] in selected_classes:\n",
    "        return {'image': example['image'], 'label': label_mapping[example['label']], 'is_selected': True}\n",
    "    else:\n",
    "        return {'is_selected': False}\n",
    "\n",
    "\n",
    "# dictorying storing each diction\n",
    "datasets: Dict[str, DatasetDict[str, Dataset]] = {}\n",
    "\n",
    "# applying everything to all 6 batches \n",
    "for batch, selected_classes in [('batcha', batcha_selected_classes_10),\n",
    "                                ('batcha', batcha_selected_classes_50),\n",
    "                                ('batchb', batchb_selected_classes_10),\n",
    "                                ('batchb', batchb_selected_classes_50),\n",
    "                                ('batchc', batchc_selected_classes_10),\n",
    "                                ('batchc', batchc_selected_classes_50)]:\n",
    "    # getting the label mapping\n",
    "    label_mapping:Dict[int, int] = create_label_mapping(selected_classes)\n",
    "    # flagging and mapping labels each class\n",
    "    dataset_with_updated_labels: DatasetDict = dataset.map(lambda x: transform_example(x, selected_classes, label_mapping))\n",
    "    \n",
    "    # filtering classes that are flagged/selected\n",
    "    # very pythonic syntax cuz train, val, test splits \n",
    "    filtered_dataset: Dict[str, Dataset] = {split: ds.filter(lambda x: x['is_selected']) for split, ds in dataset_with_updated_labels.items()}\n",
    "    datasets[f\"{batch}_{len(selected_classes)}\"] = filtered_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline \n",
    "\n",
    "- transforing dataset to tensors\n",
    "- storing everything in a dict \n",
    "- subsetting data based on batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset: DatasetDict) -> Dict[str, torch.Tensor]:\n",
    "    # pytorch preprocess\n",
    "    processor: transforms.Compose = transforms.Compose([\n",
    "        # augment stuff for later to see if it improves performance\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),  # range of values 0 - 1\n",
    "        transforms.Normalize((0.5,), (0.5,)),  # std .5, mean .5\n",
    "    ])\n",
    "    # convert each image to 3d numpy array then reshape into a (height, width)) tensor\n",
    "    images: List[torch.Tensor] = [processor(np.array(x).reshape(28, 28)) for x in dataset['image']]\n",
    "    # return dictionary containing single 3d tensor storing all images and 1d tensor for labels\n",
    "    return {'image': torch.stack(images), 'label': torch.tensor(dataset['label'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting data \n",
    "Original Data 4.5 Million rows 90/5/5 split\n",
    "Updated \n",
    "\n",
    "- 10 Classes: 100% Train. 100% Validation% 100% Testing\n",
    "\n",
    "- 50 Classes: 30% Train.  60% Validation% 60% Testing\n",
    "\n",
    "- 345 Classes: 25% Train. 50% Validation% 50% Testing\n",
    "\n",
    "- Using pytorch subset to do this!\n",
    "\n",
    "Setting up dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchA 10 classes\n",
    "transformed_dataset_batcha_10 = {split: ds.with_transform(transform) for split, ds in datasets['batcha_10'].items()}\n",
    "\n",
    "subset_ratio = 1\n",
    "train_subset_size_batcha_10 = int(subset_ratio * len(transformed_dataset_batcha_10['train']))\n",
    "subset_train_dataset_batcha_10 = Subset(transformed_dataset_batcha_10['train'], range(train_subset_size_batcha_10))\n",
    "\n",
    "subset_ratio = 1\n",
    "val_subset_size_batcha_10 = int(subset_ratio * len(transformed_dataset_batcha_10['valid']))\n",
    "subset_validation_dataset_batcha_10 = Subset(transformed_dataset_batcha_10['valid'], range(val_subset_size_batcha_10))\n",
    "\n",
    "subset_ratio = 1\n",
    "test_subset_size_batcha_10 = int(subset_ratio * len(transformed_dataset_batcha_10['test']))\n",
    "subset_test_dataset_batcha_10 = Subset(transformed_dataset_batcha_10['test'], range(test_subset_size_batcha_10))\n",
    "\n",
    "train_loader_batcha_10 = DataLoader(subset_train_dataset_batcha_10, batch_size=16, shuffle=True)\n",
    "validation_loader_batcha_10 = DataLoader(subset_validation_dataset_batcha_10, batch_size=16, shuffle=False)\n",
    "test_loader_batcha_10 = DataLoader(subset_test_dataset_batcha_10, batch_size=16, shuffle=False)\n",
    "\n",
    "# BatchB 10 classes\n",
    "transformed_dataset_batchb_10 = {split: ds.with_transform(transform) for split, ds in datasets['batchb_10'].items()}\n",
    "\n",
    "subset_ratio = 1\n",
    "train_subset_size_batchb_10 = int(subset_ratio * len(transformed_dataset_batchb_10['train']))\n",
    "subset_train_dataset_batchb_10 = Subset(transformed_dataset_batchb_10['train'], range(train_subset_size_batchb_10))\n",
    "\n",
    "subset_ratio = 1\n",
    "val_subset_size_batchb_10 = int(subset_ratio * len(transformed_dataset_batchb_10['valid']))\n",
    "subset_validation_dataset_batchb_10 = Subset(transformed_dataset_batchb_10['valid'], range(val_subset_size_batchb_10))\n",
    "\n",
    "subset_ratio = 1\n",
    "test_subset_size_batchb_10 = int(subset_ratio * len(transformed_dataset_batchb_10['test']))\n",
    "subset_test_dataset_batchb_10 = Subset(transformed_dataset_batchb_10['test'], range(test_subset_size_batchb_10))\n",
    "\n",
    "train_loader_batchb_10 = DataLoader(subset_train_dataset_batchb_10, batch_size=16, shuffle=True)\n",
    "validation_loader_batchb_10 = DataLoader(subset_validation_dataset_batchb_10, batch_size=16, shuffle=False)\n",
    "test_loader_batchb_10 = DataLoader(subset_test_dataset_batchb_10, batch_size=16, shuffle=False)\n",
    "\n",
    "# BatchC 10 classes\n",
    "transformed_dataset_batchc_10 = {split: ds.with_transform(transform) for split, ds in datasets['batchc_10'].items()}\n",
    "\n",
    "subset_ratio = 1\n",
    "train_subset_size_batchc_10 = int(subset_ratio * len(transformed_dataset_batchc_10['train']))\n",
    "subset_train_dataset_batchc_10 = Subset(transformed_dataset_batchc_10['train'], range(train_subset_size_batchc_10))\n",
    "\n",
    "subset_ratio = 1\n",
    "val_subset_size_batchc_10 = int(subset_ratio * len(transformed_dataset_batchc_10['valid']))\n",
    "subset_validation_dataset_batchc_10 = Subset(transformed_dataset_batchc_10['valid'], range(val_subset_size_batchc_10))\n",
    "\n",
    "subset_ratio = 1\n",
    "test_subset_size_batchc_10 = int(subset_ratio * len(transformed_dataset_batchc_10['test']))\n",
    "subset_test_dataset_batchc_10 = Subset(transformed_dataset_batchc_10['test'], range(test_subset_size_batchc_10))\n",
    "\n",
    "train_loader_batchc_10 = DataLoader(subset_train_dataset_batchc_10, batch_size=16, shuffle=True)\n",
    "validation_loader_batchc_10 = DataLoader(subset_validation_dataset_batchc_10, batch_size=16, shuffle=False)\n",
    "test_loader_batchc_10 = DataLoader(subset_test_dataset_batchc_10, batch_size=16, shuffle=False)\n",
    "\n",
    "# BatchA 50 classes\n",
    "transformed_dataset_batcha_50 = {split: ds.with_transform(transform) for split, ds in datasets['batcha_50'].items()}\n",
    "\n",
    "subset_ratio = 0.6\n",
    "train_subset_size_batcha_50 = int(subset_ratio * len(transformed_dataset_batcha_50['train']))\n",
    "subset_train_dataset_batcha_50 = Subset(transformed_dataset_batcha_50['train'], range(train_subset_size_batcha_50))\n",
    "\n",
    "subset_ratio = 0.3\n",
    "val_subset_size_batcha_50 = int(subset_ratio * len(transformed_dataset_batcha_50['valid']))\n",
    "subset_validation_dataset_batcha_50 = Subset(transformed_dataset_batcha_50['valid'], range(val_subset_size_batcha_50))\n",
    "\n",
    "subset_ratio = 0.3\n",
    "test_subset_size_batcha_50 = int(subset_ratio * len(transformed_dataset_batcha_50['test']))\n",
    "subset_test_dataset_batcha_50 = Subset(transformed_dataset_batcha_50['test'], range(test_subset_size_batcha_50))\n",
    "\n",
    "train_loader_batcha_50 = DataLoader(subset_train_dataset_batcha_50, batch_size=16, shuffle=True)\n",
    "validation_loader_batcha_50 = DataLoader(subset_validation_dataset_batcha_50, batch_size=16, shuffle=False)\n",
    "test_loader_batcha_50 = DataLoader(subset_test_dataset_batcha_50, batch_size=16, shuffle=False)\n",
    "\n",
    "# BatchB 50 classes\n",
    "transformed_dataset_batchb_50 = {split: ds.with_transform(transform) for split, ds in datasets['batchb_50'].items()}\n",
    "\n",
    "subset_ratio = 0.6\n",
    "train_subset_size_batchb_50 = int(subset_ratio * len(transformed_dataset_batchb_50['train']))\n",
    "subset_train_dataset_batchb_50 = Subset(transformed_dataset_batchb_50['train'], range(train_subset_size_batchb_50))\n",
    "\n",
    "subset_ratio = 0.3\n",
    "val_subset_size_batchb_50 = int(subset_ratio * len(transformed_dataset_batchb_50['valid']))\n",
    "subset_validation_dataset_batchb_50 = Subset(transformed_dataset_batchb_50['valid'], range(val_subset_size_batchb_50))\n",
    "\n",
    "subset_ratio = 0.3\n",
    "test_subset_size_batchb_50 = int(subset_ratio * len(transformed_dataset_batchb_50['test']))\n",
    "subset_test_dataset_batchb_50 = Subset(transformed_dataset_batchb_50['test'], range(test_subset_size_batchb_50))\n",
    "\n",
    "train_loader_batchb_50 = DataLoader(subset_train_dataset_batchb_50, batch_size=16, shuffle=True)\n",
    "validation_loader_batchb_50 = DataLoader(subset_validation_dataset_batchb_50, batch_size=16, shuffle=False)\n",
    "test_loader_batchb_50 = DataLoader(subset_test_dataset_batchb_50, batch_size=16, shuffle=False)\n",
    "\n",
    "# BatchC 50 classes\n",
    "transformed_dataset_batchc_50 = {split: ds.with_transform(transform) for split, ds in datasets['batchc_50'].items()}\n",
    "\n",
    "subset_ratio = 0.6\n",
    "train_subset_size_batchc_50 = int(subset_ratio * len(transformed_dataset_batchc_50['train']))\n",
    "subset_train_dataset_batchc_50 = Subset(transformed_dataset_batchc_50['train'], range(train_subset_size_batchc_50))\n",
    "\n",
    "subset_ratio = 0.3\n",
    "val_subset_size_batchc_50 = int(subset_ratio * len(transformed_dataset_batchc_50['valid']))\n",
    "subset_validation_dataset_batchc_50 = Subset(transformed_dataset_batchc_50['valid'], range(val_subset_size_batchc_50))\n",
    "\n",
    "subset_ratio = 0.3\n",
    "test_subset_size_batchc_50 = int(subset_ratio * len(transformed_dataset_batchc_50['test']))\n",
    "subset_test_dataset_batchc_50 = Subset(transformed_dataset_batchc_50['test'], range(test_subset_size_batchc_50))\n",
    "\n",
    "train_loader_batchc_50 = DataLoader(subset_train_dataset_batchc_50, batch_size=8, shuffle=True)\n",
    "validation_loader_batchc_50 = DataLoader(subset_validation_dataset_batchc_50, batch_size=8, shuffle=False)\n",
    "test_loader_batchc_50 = DataLoader(subset_test_dataset_batchc_50, batch_size=8, shuffle=False)\n",
    "\n",
    "transformed_dataset = {split: ds.with_transform(transform) for split, ds in dataset.items()}\n",
    "# subset selected\n",
    "train_subset_size_345 = int(0.25 * len(transformed_dataset['train']))\n",
    "subset_train_dataset_345 = Subset(transformed_dataset['train'], range(train_subset_size_345))\n",
    "\n",
    "val_subset_size_345 = int(0.50 * len(transformed_dataset['valid']))\n",
    "subset_validation_dataset_345 = Subset(transformed_dataset['valid'], range(val_subset_size_345))\n",
    "\n",
    "test_subset_size_345 = int(0.50 * len(transformed_dataset['test']))\n",
    "subset_test_dataset_345 = Subset(transformed_dataset['test'], range(test_subset_size_345))\n",
    "\n",
    "train_loader_345 = DataLoader(subset_train_dataset_345, batch_size=16, shuffle=True)\n",
    "validation_loader_345 = DataLoader(subset_validation_dataset_345, batch_size=16, shuffle=False)\n",
    "test_loader_345 = DataLoader(subset_test_dataset_345, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "- Models for each batch 7 models total!\n",
    "- 9 CNN models written in CNN_models.py\n",
    "    - CNN1 - A simple convolutional neural network model.\n",
    "    - CNN2 - An improved convolutional neural network model with additional layers and modifications.\n",
    "    - CNN3 - A version with more filters than the base model.\n",
    "    - CNN4 - A version with additional dense layers.\n",
    "    - CNN5 - A version with more filters.\n",
    "    - CNN6 - A version with double convolutional blocks.\n",
    "    - bestCNN - A version with hyperparameter optimization using Ray Tune.\n",
    "    - CNN8 - A version with residual blocks based on the Dive into Deep Learning textbook.\n",
    "    - CNN9 - A version with more layers of residual blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             288\n",
      "       BatchNorm2d-2           [-1, 32, 28, 28]              64\n",
      "              ReLU-3           [-1, 32, 28, 28]               0\n",
      "           Dropout-4           [-1, 32, 28, 28]               0\n",
      "            Conv2d-5           [-1, 32, 28, 28]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 28, 28]              64\n",
      "              ReLU-7           [-1, 32, 28, 28]               0\n",
      "           Dropout-8           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-9           [-1, 32, 14, 14]               0\n",
      "           Conv2d-10           [-1, 64, 14, 14]          18,432\n",
      "      BatchNorm2d-11           [-1, 64, 14, 14]             128\n",
      "             ReLU-12           [-1, 64, 14, 14]               0\n",
      "          Dropout-13           [-1, 64, 14, 14]               0\n",
      "           Conv2d-14           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 14, 14]             128\n",
      "             ReLU-16           [-1, 64, 14, 14]               0\n",
      "          Dropout-17           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-18             [-1, 64, 7, 7]               0\n",
      "           Conv2d-19            [-1, 128, 7, 7]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 7, 7]             256\n",
      "             ReLU-21            [-1, 128, 7, 7]               0\n",
      "          Dropout-22            [-1, 128, 7, 7]               0\n",
      "           Conv2d-23            [-1, 128, 7, 7]         147,456\n",
      "      BatchNorm2d-24            [-1, 128, 7, 7]             256\n",
      "             ReLU-25            [-1, 128, 7, 7]               0\n",
      "          Dropout-26            [-1, 128, 7, 7]               0\n",
      "        MaxPool2d-27            [-1, 128, 3, 3]               0\n",
      "          Dropout-28            [-1, 128, 3, 3]               0\n",
      "          Flatten-29                 [-1, 1152]               0\n",
      "           Linear-30                  [-1, 256]         295,168\n",
      "          Sigmoid-31                  [-1, 256]               0\n",
      "           Linear-32                  [-1, 345]          88,665\n",
      "================================================================\n",
      "Total params: 670,713\n",
      "Trainable params: 670,713\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.78\n",
      "Params size (MB): 2.56\n",
      "Estimated Total Size (MB): 5.35\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bestCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
       "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
       "  (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False)\n",
       "  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=1152, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=50, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout4): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CNN_models import bestCNN\n",
    "model_full = bestCNN(345)\n",
    "print(summary(model_full, (1, 28, 28)))\n",
    "model_full.to(device)\n",
    "\n",
    "model_batcha_10 = bestCNN(10)\n",
    "model_batcha_10.to(device)\n",
    "\n",
    "model_batchb_10 = bestCNN(10)\n",
    "model_batchb_10.to(device)\n",
    "\n",
    "model_batchc_10 = bestCNN(10)\n",
    "model_batchc_10.to(device)\n",
    "\n",
    "model_batcha_50 = bestCNN(50)\n",
    "model_batcha_50.to(device)\n",
    "\n",
    "model_batchb_50 = bestCNN(50)\n",
    "model_batchb_50.to(device)\n",
    "\n",
    "model_batchc_50 = bestCNN(50)\n",
    "model_batchc_50.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model \n",
    "- creating to handle training and so it's flexible for all 7 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, criterion, optimizer, device, num_epochs, writer_name, save_folder):\n",
    "    # initialize tensorboard\n",
    "    writer = SummaryWriter(writer_name)\n",
    "    \n",
    "    # training/eval loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        \n",
    "        # train mode\n",
    "        model.train()\n",
    "        \n",
    "        # process batches\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_top1 = 0\n",
    "        correct_top3 = 0\n",
    "        total = 0\n",
    "        preds_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        # validation loop\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # model predictions\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # top-1 and top-3 accuracy\n",
    "                _, predicted_top1 = torch.max(preds.data, 1)\n",
    "                _, predicted_top3 = torch.topk(preds.data, 3, dim=1)\n",
    "                total += labels.size(0)\n",
    "                preds_list.append(preds)\n",
    "                labels_list.append(labels)\n",
    "                \n",
    "                # update correct guesses\n",
    "                correct_top1 += (predicted_top1 == labels).sum().item()\n",
    "                correct_top3 += (predicted_top3 == labels.unsqueeze(1)).any(dim=1).sum().item()\n",
    "        \n",
    "        preds_tensor = torch.cat(preds_list, dim=0)\n",
    "        labels_tensor = torch.cat(labels_list, dim=0)\n",
    "        \n",
    "        # final stats\n",
    "        val_loss /= len(validation_loader)\n",
    "        val_accuracy_top1 = correct_top1 / total\n",
    "        val_accuracy_top3 = correct_top3 / total\n",
    "        \n",
    "        # additional metrics\n",
    "        preds_top1 = torch.argmax(preds_tensor, dim=1).cpu().numpy()\n",
    "        labels_numpy = labels_tensor.cpu().numpy()\n",
    "        f1 = f1_score(labels_numpy, preds_top1, average='weighted')\n",
    "        precision = precision_score(labels_numpy, preds_top1, average='weighted')\n",
    "        recall = recall_score(labels_numpy, preds_top1, average='weighted')\n",
    "        \n",
    "        # log to tensorboard\n",
    "        writer.add_scalar('Train Loss', epoch_loss, epoch)\n",
    "        writer.add_scalar('Val Loss', val_loss, epoch)\n",
    "        writer.add_scalar('Val AccuracyTop1', val_accuracy_top1, epoch)\n",
    "        writer.add_scalar('Val AccuracyTop3', val_accuracy_top3, epoch)\n",
    "        writer.add_scalar('F1 Score', f1, epoch)\n",
    "        writer.add_scalar('Precision', precision, epoch)\n",
    "        writer.add_scalar('Recall', recall, epoch)\n",
    "        \n",
    "        # print stats\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Accuracy Top1: {val_accuracy_top1:.4f}, Val Accuracy Top3: {val_accuracy_top3:.4f}, '\n",
    "              f'F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "    \n",
    "    # save model\n",
    "    save_path = f\"{save_folder}\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   1%|          | 99/9586 [00:02<04:12, 37.60batch/s, loss=0.852]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer_full\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model_full\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# training each model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_batcha_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_batcha_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader_batcha_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_batcha_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining_data/batcha_10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights/batcha_10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m train_model(model_batchb_10, train_loader_batchb_10, validation_loader_batchb_10, criterion, optimizer_batchb_10, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, writer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_data/batchb_10\u001b[39m\u001b[38;5;124m'\u001b[39m, save_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights/batchb_10\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m train_model(model_batchc_10, train_loader_batchc_10, validation_loader_batchc_10, criterion, optimizer_batchc_10, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, writer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_data/batchc_10\u001b[39m\u001b[38;5;124m'\u001b[39m, save_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights/batchc_10\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, validation_loader, criterion, optimizer, device, num_epochs, writer_name, save_folder)\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# process batches\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     15\u001b[0m     images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/torch/utils/data/dataset.py:397\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T_co]:\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/arrow_dataset.py:2814\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2814\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2815\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/arrow_dataset.py:2810\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/arrow_dataset.py:2795\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2794\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[0;32m-> 2795\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/formatting/formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/formatting/formatting.py:514\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    513\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[0;32m--> 514\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(batch)\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/formatting/formatting.py:221\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m batch\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/features/features.py:1983\u001b[0m, in \u001b[0;36mFeatures.decode_batch\u001b[0;34m(self, batch, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1980\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1981\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1982\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1983\u001b[0m         [\n\u001b[1;32m   1984\u001b[0m             decode_nested_example(\u001b[38;5;28mself\u001b[39m[column_name], value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[1;32m   1985\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   1988\u001b[0m         ]\n\u001b[1;32m   1989\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1990\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   1991\u001b[0m     )\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/features/features.py:1984\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1980\u001b[0m decoded_batch \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1981\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1982\u001b[0m     decoded_batch[column_name] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1983\u001b[0m         [\n\u001b[0;32m-> 1984\u001b[0m             \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1985\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m   1988\u001b[0m         ]\n\u001b[1;32m   1989\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[1;32m   1990\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m   1991\u001b[0m     )\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_batch\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/features/features.py:1341\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[0;32m-> 1341\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/datasets/features/image.py:184\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    182\u001b[0m             image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(bytes_)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m image\u001b[38;5;241m.\u001b[39mload()  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/PIL/Image.py:3318\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3315\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3318\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[1;32m   3321\u001b[0m     checked_formats \u001b[38;5;241m=\u001b[39m formats\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/PIL/Image.py:3304\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3302\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[1;32m   3303\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3304\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3305\u001b[0m     _decompression_bomb_check(im\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m   3306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/PIL/ImageFile.py:137\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;167;01mIndexError\u001b[39;00m,  \u001b[38;5;66;03m# end of data\u001b[39;00m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;167;01mTypeError\u001b[39;00m,  \u001b[38;5;66;03m# end of data (ord)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m         struct\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    144\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(v) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mv\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/PIL/PngImagePlugin.py:724\u001b[0m, in \u001b[0;36mPngImageFile._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    721\u001b[0m cid, pos, length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpng\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/PIL/PngImagePlugin.py:192\u001b[0m, in \u001b[0;36mChunkStream.call\u001b[0;34m(self, cid, pos, length)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the appropriate chunk handler\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTREAM \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cid, pos, length)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mascii\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/drawAI/.env/lib/python3.9/site-packages/PIL/PngImagePlugin.py:443\u001b[0m, in \u001b[0;36mPngStream.chunk_IDAT\u001b[0;34m(self, pos, length)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_idat \u001b[38;5;241m=\u001b[39m length\n\u001b[1;32m    442\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage data found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 443\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;43;01mEOFError\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizers \n",
    "optimizer_batcha_10 = torch.optim.Adam(model_batcha_10.parameters(), lr=0.001)\n",
    "optimizer_batchb_10 = torch.optim.Adam(model_batchb_10.parameters(), lr=0.001)\n",
    "optimizer_batchc_10 = torch.optim.Adam(model_batchc_10.parameters(), lr=0.001)\n",
    "optimizer_batcha_50 = torch.optim.Adam(model_batcha_50.parameters(), lr=0.001)\n",
    "optimizer_batchb_50 = torch.optim.Adam(model_batchb_50.parameters(), lr=0.001)\n",
    "optimizer_batchc_50 = torch.optim.Adam(model_batchc_50.parameters(), lr=0.001)\n",
    "optimizer_full= torch.optim.Adam(model_full.parameters(), lr=0.001)\n",
    "\n",
    "# training each model\n",
    "train_model(model_batcha_10, train_loader_batcha_10, validation_loader_batcha_10, criterion, optimizer_batcha_10, device, num_epochs=3, writer_name='training_data/batcha_10', save_folder = 'weights/batcha_10')\n",
    "\n",
    "train_model(model_batchb_10, train_loader_batchb_10, validation_loader_batchb_10, criterion, optimizer_batchb_10, device, num_epochs=5, writer_name='training_data/batchb_10', save_folder = 'weights/batchb_10')\n",
    "\n",
    "train_model(model_batchc_10, train_loader_batchc_10, validation_loader_batchc_10, criterion, optimizer_batchc_10, device, num_epochs=5, writer_name='training_data/batchc_10', save_folder = 'weights/batchc_10')\n",
    "\n",
    "train_model(model_batcha_50, train_loader_batcha_50, validation_loader_batcha_50, criterion, optimizer_batcha_50, device, num_epochs=5, writer_name='training_data/batcha_50', save_folder = 'weights/batcha_50')\n",
    "\n",
    "train_model(model_batchb_50, train_loader_batchb_50, validation_loader_batchb_50, criterion, optimizer_batchb_50, device, num_epochs=5, writer_name='training_data/batchb_50', save_folder = 'weights/batchb_50')\n",
    "\n",
    "train_model(model_batchc_50, train_loader_batchc_50, validation_loader_batchc_50, criterion, optimizer_batchc_50, device, num_epochs=5, writer_name='training_data/batchc_50', save_folder = 'weights/batchc_50')\n",
    "\n",
    "train_model(model_full, train_loader_345, validation_loader_345, criterion, optimizer_full, device, num_epochs=5, writer_name='training_data/', save_folder = 'weights/345')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamter tuning with raytune!\n",
    "- raytune will parallize the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TuneCNN, self).__init__()\n",
    "\n",
    "        # cnn block 1\n",
    "        self.conv1 = nn.Conv2d(1, config[\"channels_1\"], kernel_size=config[\"kernal\"], padding='same', bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(config[\"channels_1\"])\n",
    "        self.conv2 = nn.Conv2d(config[\"channels_1\"], config[\"channels_1\"], kernel_size=config[\"kernal\"], padding='same', bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(config[\"channels_1\"])\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.dropout1 = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "        # cnn block 2\n",
    "        self.conv3 = nn.Conv2d(config[\"channels_1\"], config[\"channels_2\"], kernel_size=config[\"kernal\"], padding='same', bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(config[\"channels_2\"])\n",
    "        self.conv4 = nn.Conv2d(config[\"channels_2\"], config[\"channels_2\"], kernel_size=config[\"kernal\"], padding='same', bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(config[\"channels_2\"])\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.dropout2 = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "        # cnn block 3\n",
    "        self.conv5 = nn.Conv2d(config[\"channels_2\"], config[\"channels_3\"], kernel_size=config[\"kernal\"], padding='same', bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(config[\"channels_3\"])\n",
    "        self.conv6 = nn.Conv2d(config[\"channels_3\"], config[\"channels_3\"], kernel_size=config[\"kernal\"], padding='same', bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(config[\"channels_3\"])\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.dropout3 = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "        # calculate output size after pooling layers\n",
    "        input_size = 28\n",
    "        output_size = input_size // 2  # after first pooling layer\n",
    "        output_size //= 2  # after second pooling layer\n",
    "        output_size //= 2  # after third pooling layer\n",
    "\n",
    "        # fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(config[\"channels_3\"] * output_size * output_size, config[\"hidden\"])\n",
    "        self.fc2 = nn.Linear(config[\"hidden\"], config[\"hidden\"])\n",
    "        self.fc3 = nn.Linear(config[\"hidden\"], 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        if config[\"activation\"] == \"ReLU\":\n",
    "            self.tanh = nn.ReLU()\n",
    "        elif config[\"activation\"] == \"Sigmoid\":\n",
    "            self.tanh = nn.Sigmoid()\n",
    "        else:\n",
    "            self.tanh = nn.Tanh()\n",
    "        self.dropout4 = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.tanh(self.fc1(x))\n",
    "        x = self.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 21:19:04,527\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "2024-04-26 21:19:05,209\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run(...)`.\n",
      "2024-04-26 21:19:05,226\tINFO tune.py:657 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-26 21:19:37</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:32.52        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.5/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">  kernal</th><th style=\"text-align: right;\">         lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>tune_model_238a1_00000</td><td>RUNNING </td><td>127.0.0.1:14790</td><td>ReLU        </td><td style=\"text-align: right;\"> 0.31604 </td><td style=\"text-align: right;\">       3</td><td style=\"text-align: right;\">0.000571614</td></tr>\n",
       "<tr><td>tune_model_238a1_00001</td><td>RUNNING </td><td>127.0.0.1:14789</td><td>Tanh        </td><td style=\"text-align: right;\"> 0.328681</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.000184132</td></tr>\n",
       "<tr><td>tune_model_238a1_00002</td><td>RUNNING </td><td>127.0.0.1:14791</td><td>Tanh        </td><td style=\"text-align: right;\"> 0.337156</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.000279503</td></tr>\n",
       "<tr><td>tune_model_238a1_00003</td><td>RUNNING </td><td>127.0.0.1:14792</td><td>Tanh        </td><td style=\"text-align: right;\"> 0.394202</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.000210923</td></tr>\n",
       "<tr><td>tune_model_238a1_00004</td><td>PENDING </td><td>               </td><td>ReLU        </td><td style=\"text-align: right;\"> 0.108964</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.00130743 </td></tr>\n",
       "<tr><td>tune_model_238a1_00005</td><td>PENDING </td><td>               </td><td>Tanh        </td><td style=\"text-align: right;\"> 0.239099</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.00430575 </td></tr>\n",
       "<tr><td>tune_model_238a1_00006</td><td>PENDING </td><td>               </td><td>Sigmoid     </td><td style=\"text-align: right;\"> 0.386518</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.00250953 </td></tr>\n",
       "<tr><td>tune_model_238a1_00007</td><td>PENDING </td><td>               </td><td>Tanh        </td><td style=\"text-align: right;\"> 0.45088 </td><td style=\"text-align: right;\">       3</td><td style=\"text-align: right;\">0.000106302</td></tr>\n",
       "<tr><td>tune_model_238a1_00008</td><td>PENDING </td><td>               </td><td>Tanh        </td><td style=\"text-align: right;\"> 0.117641</td><td style=\"text-align: right;\">       3</td><td style=\"text-align: right;\">0.00931059 </td></tr>\n",
       "<tr><td>tune_model_238a1_00009</td><td>PENDING </td><td>               </td><td>Tanh        </td><td style=\"text-align: right;\"> 0.293567</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.00134919 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m /Users/kelley/Desktop/drawAI/.env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "Epoch 1/3:   0%|          | 0/9586 [00:00<?, ?batch/s]\n",
      "Epoch 1/3:   0%|          | 1/9586 [00:00<1:47:37,  1.48batch/s, loss=1.11]\n",
      "Epoch 1/3:   0%|          | 4/9586 [00:00<25:00,  6.39batch/s, loss=1.22]   \n",
      "Epoch 1/3:   0%|          | 1/9586 [00:00<1:51:25,  1.43batch/s, loss=0.686]\n",
      "Epoch 1/3:   0%|          | 9/9586 [00:00<10:52, 14.68batch/s, loss=0.703]\n",
      "Epoch 1/3:   0%|          | 10/9586 [00:00<10:08, 15.75batch/s, loss=1.25] \n",
      "Epoch 1/3:   0%|          | 14/9586 [00:01<07:40, 20.77batch/s, loss=0.636]\n",
      "Epoch 1/3:   0%|          | 18/9586 [00:01<06:16, 25.39batch/s, loss=1.12] \n",
      "Epoch 1/3:   1%|          | 74/9586 [00:02<03:42, 42.80batch/s, loss=0.988]\n",
      "Epoch 1/3:   0%|          | 0/9586 [00:00<?, ?batch/s]\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Epoch 1/3:   2%|▏         | 145/9586 [00:05<06:22, 24.66batch/s, loss=0.581]\n",
      "Epoch 1/3:   0%|          | 1/9586 [00:00<2:01:30,  1.31batch/s, loss=0.736]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1/3:   1%|▏         | 126/9586 [00:04<07:29, 21.03batch/s, loss=0.854]\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "Epoch 1/3:   2%|▏         | 163/9586 [00:05<04:19, 36.35batch/s, loss=0.789]\u001b[32m [repeated 45x across cluster]\u001b[0m\n",
      "Epoch 1/3:   2%|▏         | 173/9586 [00:06<04:01, 38.99batch/s, loss=1.44]\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "Epoch 1/3:   2%|▏         | 168/9586 [00:05<04:08, 37.84batch/s, loss=0.842]\u001b[32m [repeated 35x across cluster]\u001b[0m\n",
      "Epoch 1/3:   2%|▏         | 148/9586 [00:05<06:44, 23.35batch/s, loss=0.495]\u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "Epoch 1/3:   1%|▏         | 143/9586 [00:05<08:05, 19.46batch/s, loss=0.99] \u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "Epoch 1/3:   4%|▎         | 358/9586 [00:10<03:28, 44.17batch/s, loss=0.716]\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "Epoch 1/3:   4%|▍         | 388/9586 [00:11<03:26, 44.60batch/s, loss=0.422]\u001b[32m [repeated 146x across cluster]\u001b[0m\n",
      "Epoch 1/3:   4%|▍         | 385/9586 [00:10<03:23, 45.17batch/s, loss=0.385]\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
      "Epoch 1/3:   4%|▍         | 385/9586 [00:10<03:23, 45.17batch/s, loss=0.674]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 1/3:   4%|▎         | 358/9586 [00:10<03:28, 44.17batch/s, loss=0.936]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1/3:   5%|▌         | 493/9586 [00:13<03:34, 42.39batch/s, loss=0.273]\n",
      "Epoch 1/3:   6%|▌         | 551/9586 [00:14<04:04, 37.00batch/s, loss=0.495]\n",
      "Epoch 1/3:   6%|▌         | 586/9586 [00:15<03:27, 43.31batch/s, loss=0.65] \u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "Epoch 1/3:   6%|▋         | 606/9586 [00:16<03:39, 40.83batch/s, loss=0.605]\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
      "Epoch 1/3:   6%|▋         | 603/9586 [00:16<03:49, 39.10batch/s, loss=0.413]\u001b[32m [repeated 80x across cluster]\u001b[0m\n",
      "Epoch 1/3:   6%|▌         | 580/9586 [00:15<03:28, 43.21batch/s, loss=0.473]\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "Epoch 1/3:   6%|▌         | 571/9586 [00:15<03:30, 42.75batch/s, loss=0.541] \n",
      "Epoch 1/3:   7%|▋         | 661/9586 [00:17<03:37, 40.97batch/s, loss=0.42] \n",
      "Epoch 1/3:   6%|▌         | 543/9586 [00:14<03:57, 38.12batch/s, loss=0.478]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1/3:   6%|▌         | 566/9586 [00:15<03:40, 40.83batch/s, loss=0.481]\n",
      "Epoch 1/3:   8%|▊         | 806/9586 [00:20<03:37, 40.30batch/s, loss=0.495]\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "Epoch 1/3:   8%|▊         | 810/9586 [00:20<03:35, 40.78batch/s, loss=0.213]\u001b[32m [repeated 63x across cluster]\u001b[0m\n",
      "Epoch 1/3:   8%|▊         | 813/9586 [00:21<03:42, 39.39batch/s, loss=0.423]\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
      "Epoch 1/3:   8%|▊         | 813/9586 [00:21<03:42, 39.39batch/s, loss=0.346]\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "Epoch 1/3:   8%|▊         | 731/9586 [00:18<03:30, 42.04batch/s, loss=0.199]\n",
      "Epoch 1/3:   9%|▉         | 874/9586 [00:22<03:25, 42.41batch/s, loss=0.437]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "Epoch 1/3:   9%|▉         | 910/9586 [00:23<03:44, 38.73batch/s, loss=0.104]\n",
      "Epoch 1/3:  10%|█         | 978/9586 [00:25<03:25, 41.87batch/s, loss=0.431]\n",
      "Epoch 1/3:  11%|█         | 1009/9586 [00:25<03:14, 44.08batch/s, loss=0.467]\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Epoch 1/3:  11%|█         | 1008/9586 [00:26<03:19, 42.99batch/s, loss=0.137]\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
      "Epoch 1/3:  11%|█         | 1013/9586 [00:26<03:15, 43.82batch/s, loss=0.441]\u001b[32m [repeated 75x across cluster]\u001b[0m\n",
      "Epoch 1/3:  11%|█         | 1036/9586 [00:26<03:08, 45.36batch/s, loss=0.243]\n",
      "Epoch 1/3:  11%|█         | 1034/9586 [00:26<03:10, 44.99batch/s, loss=1]    \u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "2024-04-26 21:19:37,803\tWARNING tune.py:192 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "Epoch 1/3:  11%|█         | 1058/9586 [00:27<03:09, 44.89batch/s, loss=0.479] \n",
      "Epoch 1/3:  11%|█         | 1063/9586 [00:27<03:06, 45.75batch/s, loss=0.411]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Epoch 1/3:  12%|█▏        | 1103/9586 [00:28<03:21, 42.02batch/s, loss=0.62]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "Epoch 1/3:  13%|█▎        | 1208/9586 [00:31<03:16, 42.68batch/s, loss=0.245]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "Epoch 1/3:  13%|█▎        | 1236/9586 [00:30<03:16, 42.40batch/s, loss=0.624]\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
      "Epoch 1/3:  13%|█▎        | 1241/9586 [00:31<03:22, 41.20batch/s, loss=0.668]\u001b[32m [repeated 67x across cluster]\u001b[0m\n",
      "Epoch 1/3:  12%|█▏        | 1163/9586 [00:29<03:41, 37.95batch/s, loss=0.147]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Epoch 1/3:  13%|█▎        | 1241/9586 [00:31<03:22, 41.20batch/s, loss=0.98] \u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "Epoch 1/3:  13%|█▎        | 1292/9586 [00:33<04:13, 32.67batch/s, loss=0.0558]\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
      "Epoch 1/3:  13%|█▎        | 1288/9586 [00:33<04:03, 34.07batch/s, loss=0.0928]\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "Epoch 1/3:  15%|█▍        | 1406/9586 [00:35<03:15, 41.77batch/s, loss=0.137]\u001b[32m [repeated 52x across cluster]\u001b[0m\n",
      "Epoch 1/3:  15%|█▍        | 1402/9586 [00:36<03:10, 42.89batch/s, loss=0.274]\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
      "Epoch 1/3:  15%|█▍        | 1392/9586 [00:36<03:09, 43.17batch/s, loss=0.825]\u001b[32m [repeated 73x across cluster]\u001b[0m\n",
      "Epoch 1/3:  15%|█▍        | 1406/9586 [00:35<03:14, 42.01batch/s, loss=0.407]\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "2024-04-26 21:19:47,891\tINFO tune.py:1148 -- Total run time: 42.66 seconds (32.52 seconds for the tuning loop).\n",
      "2024-04-26 21:19:47,902\tWARNING tune.py:1163 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "2024-04-26 21:19:47,990\tWARNING experiment_analysis.py:916 -- Failed to read the results for 10 trials:\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00000_0_activation=ReLU,dropout=0.3160,kernal=3,lr=0.0006_2024-04-26_21-19-06\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00001_1_activation=Tanh,dropout=0.3287,kernal=5,lr=0.0002_2024-04-26_21-19-06\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00002_2_activation=Tanh,dropout=0.3372,kernal=5,lr=0.0003_2024-04-26_21-19-06\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00003_3_activation=Tanh,dropout=0.3942,kernal=5,lr=0.0002_2024-04-26_21-19-06\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00004_4_activation=ReLU,dropout=0.1090,kernal=5,lr=0.0013_2024-04-26_21-19-06\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00005_5_activation=Tanh,dropout=0.2391,kernal=5,lr=0.0043_2024-04-26_21-19-06\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00006_6_activation=Sigmoid,dropout=0.3865,kernal=5,lr=0.0025_2024-04-26_21-19-06\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00007_7_activation=Tanh,dropout=0.4509,kernal=3,lr=0.0001_2024-04-26_21-19-06\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00008_8_activation=Tanh,dropout=0.1176,kernal=3,lr=0.0093_2024-04-26_21-19-07\n",
      "- /Users/kelley/ray_results/tune_model_2024-04-26_21-19-05/tune_model_238a1_00009_9_activation=Tanh,dropout=0.2936,kernal=5,lr=0.0013_2024-04-26_21-19-07\n",
      "2024-04-26 21:19:47,992\tWARNING experiment_analysis.py:783 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(tune_model pid=14790)\u001b[0m /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(tune_model pid=14790)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "# creating configuration dictionary!\n",
    "config = {\n",
    "    \"channels_1\": 32,\n",
    "    \"channels_2\": 64,\n",
    "    \"channels_3\": 128,\n",
    "    \"kernal\": tune.choice([3, 5]),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "    \"hidden\": 256,\n",
    "    \"activation\": tune.choice([\"ReLU\", \"Sigmoid\", \"Tanh\"]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),\n",
    "}\n",
    "\n",
    "# two funcitons necessary to prevent rewritting calling\n",
    "# calling original training function\n",
    "def train_model_new(config, model, train_loader, validation_loader, criterion, device, num_epochs, writer_name, save_folder):\n",
    "    # create the optimizer based on the configuration\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    # call the original train_model function with the configured optimizer\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        validation_loader=validation_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        writer_name=writer_name,\n",
    "        save_folder=save_folder\n",
    "    )\n",
    "\n",
    "def tune_model(config):\n",
    "    # create the model based on the configuration\n",
    "    model = TuneCNN(config)\n",
    "    model.to(device)\n",
    "\n",
    "    # call the train_model function with the configured model and other arguments\n",
    "    train_model_new(\n",
    "        config=config,\n",
    "        model=model_batcha_10,\n",
    "        train_loader=train_loader_batcha_10,\n",
    "        validation_loader=validation_loader_batcha_10,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        num_epochs=3,\n",
    "        writer_name='training_data/batcha_10',\n",
    "        save_folder='weights/batcha_10'\n",
    "    )\n",
    "\n",
    "analysis = tune.run(\n",
    "    tune_model,\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    resources_per_trial={\"cpu\": 2},\n",
    ")\n",
    "\n",
    "best_config = analysis.get_best_config(metric=\"accuracy\", mode=\"max\")\n",
    "print(\"Best configuration: \", best_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
